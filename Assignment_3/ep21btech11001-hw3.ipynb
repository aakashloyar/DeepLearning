{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/aakash/.local/lib/python3.10/site-packages (2.1.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: matplotlib in /home/aakash/.local/lib/python3.10/site-packages (3.9.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aakash/.local/lib/python3.10/site-packages (from matplotlib) (24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aakash/.local/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aakash/.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aakash/.local/lib/python3.10/site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/aakash/.local/lib/python3.10/site-packages (from matplotlib) (2.1.1)\n",
      "Requirement already satisfied: pillow>=8 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aakash/.local/lib/python3.10/site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aakash/.local/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: scikit-learn in /home/aakash/.local/lib/python3.10/site-packages (1.5.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/aakash/.local/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/aakash/.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/aakash/.local/lib/python3.10/site-packages (from scikit-learn) (2.1.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/aakash/.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n"
     ]
    }
   ],
   "source": [
    "#installing and importing important library\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will implement a simple Artificial Neural Network (ANN) from scratch (i.e.,\n",
    "without using built-in functions). Implement the back-propagation algorithm to learn the weights of an\n",
    "ANN with 2 input nodes, 2 hidden nodes and 1 output node. The hidden layer nodes employ a sigmoid\n",
    "nonlinearity. Use squared-error loss. Train your network to learn the following binary operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XOR (10)\n",
    "2. AND (10)\n",
    "3. OR (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here we will declare size of neural netork\n",
    "#hidden size=3, input size=2,output size=1\n",
    "P=2\n",
    "M=3\n",
    "K=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this shell we initialize the input with gaussian noise and corresponding output for each operation\n",
    "def gen_data(operation,size):\n",
    "    noise=0.1\n",
    "    X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    if operation==\"XOR\":\n",
    "        Y=np.array([[0],[1],[1],[0]])\n",
    "    elif operation==\"AND\":\n",
    "        Y= np.array([[0],[0],[0],[1]])\n",
    "    elif operation==\"OR\":\n",
    "        Y=np.array([[0],[1],[1],[1]])\n",
    "    else:\n",
    "        raise ValueError(\"Wrong Operation\")\n",
    "    \n",
    "    X_gen,Y_gen=[],[]\n",
    "    for i in range(0,size):\n",
    "        ind=np.random.randint(0,4)\n",
    "        x=X[ind][0]+np.random.rand()*noise\n",
    "        y=X[ind][1]+np.random.rand()*noise\n",
    "        temp=[]\n",
    "        temp.append(x)\n",
    "        temp.append(y)\n",
    "        X_gen.append(temp)\n",
    "        Y_gen.append([Y[ind]])\n",
    "    X_gen=np.array(X_gen)\n",
    "    Y_gen=np.array(Y_gen)    \n",
    "    return X_gen,Y_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here we are declaring sigmoid function\n",
    "# sigmoid(x)=1/(1+e**(-1))\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here this function will give sigmoid derivative\n",
    "def der_sigmoid(x):\n",
    "    s = sigmoid(x)\n",
    "    return (1-s)*s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here is our loss function\n",
    "#now here we have mean of (y-yhat)**2 mean\n",
    "def Loss(Y,Y_hat):\n",
    "    sum=0\n",
    "    for i in range(0,len(Y)):\n",
    "        for k in range(0,K):\n",
    "            sum+=(Y[i][k]-Y_hat[i][k])**2\n",
    "    sum/=len(Y)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate accuracy -> correct output/total ouptput\n",
    "def Accuracy(Y, Y_hat):\n",
    "        c=0\n",
    "        for i in range(0,len(Y)):\n",
    "            Y_round=np.round(Y_hat[i][0])\n",
    "            if Y_round==Y[i][0]:\n",
    "                c=c+1\n",
    "        return c/len(Y)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in shell we will divide our input in test and training-> in 20%:80% -> nearly 1:3\n",
    "# this one is the function that give use 4 outputs-> X_train,Y_train, X_test,Y_test\n",
    "def split_data(X,Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    return X_train,Y_train,X_test,Y_test\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here in this shell we will declare our ANN\n",
    "class ANN:\n",
    "\n",
    "    #function init our ANN with eta as learning rate ->0.1\n",
    "    def __init__(ann):\n",
    "        # step1-> we have 3 hidden layer here and 2 input layer so here we will have alpha matrix of size 3*2\n",
    "        # step-> beta matrix 1*3\n",
    "        ann.alpha=np.random.rand(M,P)\n",
    "        ann.beta=np.random.rand(K,M)\n",
    "\n",
    "        ann.eta=0.1\n",
    "    \n",
    "    #function give yhat\n",
    "    def yhat(ann,X_i):\n",
    "        #step1-> we have to first get z\n",
    "        z=np.dot(X_i,ann.alpha.T)# 1*p with m*p|p*m-> 1*m\n",
    "        # so here z is 1*m\n",
    "        #step2-> apply sigmoid\n",
    "        z=sigmoid(z)\n",
    "        #step->3 we have to get yhat\n",
    "        # 1*m with k*m-> take transpose to make m*k\n",
    "        yhat=np.dot(z,ann.beta.T)\n",
    "        #giving us a 1*k\n",
    "        #step->4 apply sigmoid\n",
    "        yhat=sigmoid(yhat)\n",
    "        # print(yhat.shape)\n",
    "        #yhat is a 1*1 list\n",
    "        return yhat\n",
    "    \n",
    "    #function give array of yhat\n",
    "    def output_array(ann,X):\n",
    "        Y_hat=[]\n",
    "        for i in range(len(X)):\n",
    "            y=ann.yhat(X[i])\n",
    "            Y_hat.append(y)\n",
    "        Y_hat=np.array(Y_hat)        \n",
    "        return Y_hat    \n",
    "\n",
    "    def func1(ann,X_i,m):#-> return integer\n",
    "        # now here we have X of 1*p\n",
    "        #ann.alpha[m]-> is 1*p\n",
    "        # print(X_i.shape)\n",
    "        # print(ann.alpha[m].T.shape)\n",
    "        arr=np.dot(X_i,ann.alpha[m].T)#-> gives a scalar\n",
    "        #arr is 1*1 arr\n",
    "        # print(arr)\n",
    "        return arr\n",
    "\n",
    "    def beta_gradient(ann,X,Y,Y_hat,i,k,m):\n",
    "        res=-2*(Y[i][k]-Y_hat[i][k])*der_sigmoid(ann.beta[k][m]*ann.func1(X[i],m))*ann.func1(X[i],m)\n",
    "        return res\n",
    "    \n",
    "    #beta(k.z) z=np.dot(X(1*p),ann.alpha(m*p).T->1*m   ann.beta[k](1*m).T\n",
    "    def func2(ann,X_i,k):\n",
    "        # here X_i is (P) and alpha is (M,P)|(P,M)\n",
    "        z=np.dot(X_i,ann.alpha.T)\n",
    "        y=np.dot(z,ann.beta[k])\n",
    "        return y\n",
    "    \n",
    "    def func3(ann,X_i,m):\n",
    "        z=np.dot(X_i,ann.alpha[m])\n",
    "        return z\n",
    "    \n",
    "\n",
    "    def alpha_gradient(ann,X,Y,Y_hat,i,k,m,p):\n",
    "        res=-2*(Y[i][k]-Y_hat[i][k])*der_sigmoid(ann.func2(X[i],k))*ann.beta[k][m]*der_sigmoid(ann.func3(X[i],m))*X[i][p]\n",
    "        return res\n",
    "\n",
    "    def update(ann,X,Y,Y_hat):\n",
    "        n=len(X)\n",
    "        alpha=np.zeros((M,P))\n",
    "        beta=np.zeros((K,M))\n",
    "        for k in range(0,K):\n",
    "            for m in range(0,M):\n",
    "                gradsum=0\n",
    "                for i in range(0,n):\n",
    "                    gradsum+=ann.beta_gradient(X,Y,Y_hat,i,k,m)\n",
    "                gradsum/=n\n",
    "                beta[k][m]=ann.beta[k][m]-ann.eta*gradsum\n",
    "\n",
    "        for m in range(0,M):\n",
    "            for p in range(0,P):\n",
    "                gradsum=0\n",
    "                for i in range(0,n):\n",
    "                    for k in range(0,K):\n",
    "                        gradsum+=ann.alpha_gradient(X,Y,Y_hat,i,k,m,p)\n",
    "                gradsum/=n\n",
    "                alpha[m][p]=ann.alpha[m][p]-ann.eta*gradsum\n",
    "\n",
    "        # print(ann.alpha)\n",
    "        # print(ann.beta)        \n",
    "        # print(\"gap\")\n",
    "        ann.alpha=np.array(alpha)\n",
    "        ann.beta=np.array(beta)\n",
    "        \n",
    "        # print(ann.alpha)\n",
    "        # print(ann.beta)\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here we will train our model\n",
    "def train(operation,epochs,size):\n",
    "\n",
    "    #generating data\n",
    "    X,Y=gen_data(operation,size)\n",
    "    # print(f'X={X}')\n",
    "    # print(f'Y={Y}')\n",
    "    #splitting data\n",
    "    X_train,Y_train,X_test,Y_test=split_data(X,Y)\n",
    "    # print(f'X={X_train}')\n",
    "    # print(f'Y={Y_train}')\n",
    "    \n",
    "    # print(f'X={X_test}')\n",
    "    # print(f'Y={Y_test}')\n",
    "\n",
    "    #declaring loss and accuracy arrays\n",
    "    loss_train,accuracy_train,loss_test,accuracy_test=[],[],[],[]\n",
    "    \n",
    "    #initialising ann\n",
    "    ann1=ANN()\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        #Now Training Data\n",
    "        #Output for training data\n",
    "        Y_hat_train=ann1.output_array(X_train)\n",
    "        # print(Y_hat_train)\n",
    "        #loss for training data\n",
    "        loss=Loss(Y_train,Y_hat_train)\n",
    "\n",
    "        #accuracy for training data\n",
    "        accuracy=Accuracy(Y_train,Y_hat_train)\n",
    "\n",
    "        #appending in res array\n",
    "        loss_train.append(loss)\n",
    "        accuracy_train.append(accuracy)\n",
    "\n",
    "        #Now test data\n",
    "        #Output for test data\n",
    "        Y_hat_test=ann1.output_array(X_test)\n",
    "\n",
    "        #loss for test data\n",
    "        loss=Loss(Y_test,Y_hat_test)\n",
    "\n",
    "\n",
    "        #accuracy for test data\n",
    "        accuracy=Accuracy(Y_test,Y_hat_test)\n",
    "        \n",
    "        #appending in the res array\n",
    "        loss_test.append(loss)\n",
    "        accuracy_test.append(accuracy)\n",
    "\n",
    "        # print(Y_hat_train)\n",
    "        # print(f'X={xor_accuracy_train}')\n",
    "\n",
    "        #now moving forward-> updating our parameters\n",
    "        ann1.update(X_train,Y_train,Y_hat_train)\n",
    "        \n",
    "\n",
    "    return loss_train,accuracy_train,loss_test,accuracy_test\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph(operation,type,y):\n",
    "    x = np.arange(1, len(y)+1)\n",
    "    plt.scatter(x,y)\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel(\"No. of iterations\")\n",
    "    plt.ylabel(type)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(operation):\n",
    "    loss_train,accuracy_train,loss_test,accuracy_test=train(operation,1000,1000)\n",
    "    print(f'Loss_train={loss_train}')\n",
    "    print(f'Loss_train={accuracy_train}')\n",
    "    print(f'Loss_train={loss_test}')\n",
    "    print(f'Loss_train={accuracy_test}')\n",
    "    graph(operation,\"Training loss\",loss_train)\n",
    "    graph(operation,\"Training accuracy\",accuracy_train)\n",
    "    graph(operation,\"Test loss\",loss_test)\n",
    "    graph(operation,\"Test accuracy\",accuracy_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5815/4135526253.py:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  beta[k][m]=ann.beta[k][m]-ann.eta*gradsum\n",
      "/tmp/ipykernel_5815/4135526253.py:88: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  alpha[m][p]=ann.alpha[m][p]-ann.eta*gradsum\n"
     ]
    }
   ],
   "source": [
    "model(\"XOR\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
