{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/aakash/anaconda3/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/aakash/anaconda3/lib/python3.12/site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/aakash/anaconda3/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/aakash/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in /home/aakash/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/aakash/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/aakash/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/aakash/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/aakash/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n"
     ]
    }
   ],
   "source": [
    "#installing and importing important library\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will implement a simple Artificial Neural Network (ANN) from scratch (i.e.,\n",
    "without using built-in functions). Implement the back-propagation algorithm to learn the weights of an\n",
    "ANN with 2 input nodes, 2 hidden nodes and 1 output node. The hidden layer nodes employ a sigmoid\n",
    "nonlinearity. Use squared-error loss. Train your network to learn the following binary operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. XOR (10)\n",
    "2. AND (10)\n",
    "3. OR (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here we will declare size of neural netork\n",
    "#hidden size=3, input size=2,output size=1\n",
    "P=2;M=3;K=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this shell we initialize the input with gaussian noise and corresponding output for each operation\n",
    "def gen_data(operation,size):\n",
    "    noise=0.5\n",
    "    X=np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "    X=np.float(X)\n",
    "    if operation==\"XOR\":\n",
    "        Y=np.array([[0],[1],[1],[0]])\n",
    "    elif operation==\"AND\":\n",
    "        Y= np.array([[0],[0],[0],[1]])\n",
    "    elif operation==\"OR\":\n",
    "        Y=np.array([[0],[1],[1],[1]])\n",
    "    else:\n",
    "        raise ValueError(\"Wrong Operation\");\n",
    "    Y=np.float(Y)\n",
    "    X_gen,Y_gen=[],[]\n",
    "    for i in range(0,size):\n",
    "        ind=np.random.randint(0,4)\n",
    "        # x=X[ind][0]+np.random.rand()*noise\n",
    "        # y=X[ind][1]+np.random.rand()*noise\n",
    "        # temp=[];\n",
    "        # temp.append(x);\n",
    "        # temp.append(y);\n",
    "        temp[0] += noise * np.random.rand()\n",
    "        \n",
    "        temp[1] += noise * np.random.rand()\n",
    "\n",
    "        # # print(f\"Original temp before noise: {temp}\")  # Debug print\n",
    "        # # temp=X[ind];\n",
    "        # # temp[0]+=noise*np.random.rand();\n",
    "        # # temp[1]+=noise*np.random.rand();\n",
    "        # print(f\"Original temp before noise: {temp}\")  # Debug print\n",
    "\n",
    "        X_gen.append(temp);\n",
    "        Y_gen.append([Y[ind]]);\n",
    "\n",
    "    return X_gen,Y_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here we are declaring sigmoid function\n",
    "# sigmoid(x)=1/(1+e**(-1))\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now here this function will give sigmoid derivative\n",
    "def der_sigmoid(x):\n",
    "    return (1-sigmoid(x))*sigmoid(x);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here is our loss function\n",
    "#now here we have mean of (y-yhat)**2 mean\n",
    "def Loss(Y,Y_hat):\n",
    "    sum=0\n",
    "    for i in range(0,len(Y)):\n",
    "        for k in range(0,K):\n",
    "            sum+=(Y[i][k]-Y_hat[i][k])**2\n",
    "    sum/=len(Y)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate accuracy -> correct output/total ouptput\n",
    "def Accuracy(Y, Y_hat):\n",
    "        c=0\n",
    "        for i in range(0,len(Y)):\n",
    "            Y_round=np.round(Y_hat[i][0])\n",
    "            if Y_round==Y[i][0]:\n",
    "                c=c+1\n",
    "        return c/len(Y);        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now in shell we will divide our input in test and training-> in 20%:80% -> nearly 1:3\n",
    "# this one is the function that give use 4 outputs-> X_train,Y_train, X_test,Y_test\n",
    "def split_data(X,Y):\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "    return X_train,Y_train,X_test,Y_test\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here in this shell we will declare our ANN\n",
    "class ANN:\n",
    "\n",
    "    #function init our ANN with eta as learning rate ->0.1\n",
    "    def __init__(ann):\n",
    "        # step1-> we have 3 hidden layer here and 2 input layer so here we will have alpha matrix of size 3*2\n",
    "        ann.alpha=np.random.rand(M,P)\n",
    "        # step-> beta matrix 1*3\n",
    "        ann.beta=np.random.rand(K,M)\n",
    "        ann.eta=0.1\n",
    "    \n",
    "    #function give yhat\n",
    "    def yhat(ann,X):\n",
    "        #step1-> we have to first get z\n",
    "        z=np.dot(X,ann.alpha.T)# 1*p with p*m-> 1*m\n",
    "        # so here z is 1*m\n",
    "        #step2-> apply sigmoid\n",
    "        z=sigmoid(z)\n",
    "        #step->3 we have to get yhat\n",
    "        # 1*m with k*m-> take transpose to make m*k\n",
    "        yhat=np.dot(z,ann.beta.T)\n",
    "        #giving us a 1*k\n",
    "        #step->4 apply sigmoid\n",
    "        yhat=sigmoid(yhat)\n",
    "        return yhat\n",
    "    \n",
    "    #function give array of yhat\n",
    "    def output_array(ann,X):\n",
    "        Y_hat=[];\n",
    "        for i in range(len(X)):\n",
    "            y=ann.yhat(X[i])\n",
    "            Y_hat.append([y])\n",
    "        return Y_hat;    \n",
    "\n",
    "    def z_m(ann,X,m):\n",
    "        # now here we have X of 1*p\n",
    "        #ann.alpha[m]-> is 1*p\n",
    "        return np.dot(X,ann.alpha[m].T);\n",
    "\n",
    "    def beta_gradient(ann,X,Y,Y_hat,i,k,m):\n",
    "        X_i=X[i]\n",
    "        res=-2*(Y[i][k]-Y_hat[i][k])*der_sigmoid(ann.beta[k][m]*ann.z_m(X[i],m))*ann.z_m(X[i],m);\n",
    "        return res\n",
    "    \n",
    "    #beta(k.z) z=np.dot(X(1*p),ann.alpha(m*p).T->1*m   ann.beta[k](1*m).T\n",
    "    def func1(ann,X,k,m):\n",
    "        z=np.dot(X,ann.alpha.T)\n",
    "        y=np.dot(z,ann.beta[k])\n",
    "        return y;\n",
    "    \n",
    "    def func2(ann,X,m):\n",
    "        z=np.dot(X,ann.alpha[m].T);\n",
    "        return z;\n",
    "\n",
    "    def alpha_gradient(ann,X,Y,Y_hat,i,k,m,p):\n",
    "        X_i=X[i]\n",
    "        res=-2*(Y[i][k]-Y_hat[i][k])*der_sigmoid(ann.func1(X[i],k,m))*ann.beta[k][m]*der_sigmoid(ann.func2(X[i],m))*X[i][p];\n",
    "        return res\n",
    "\n",
    "    def update(ann,X,Y,Y_hat):\n",
    "        n=len(X)\n",
    "        alpha=np.zeros((M,P));\n",
    "        beta=np.zeros((K,M));\n",
    "        for k in range(0,K):\n",
    "            for m in range(0,M):\n",
    "                sum=0;\n",
    "                for i in range(0,n):\n",
    "                    sum+=ann.beta_gradient(X,Y,Y_hat,i,k,m);\n",
    "                sum/=n;\n",
    "                beta[k][m]=ann.beta[k][m]-ann.eta*sum;\n",
    "\n",
    "        for m in range(0,M):\n",
    "            for p in range(0,P):\n",
    "                sum=0;\n",
    "                for i in range(0,n):\n",
    "                    for k in range(0,K):\n",
    "                        sum+=ann.alpha_gradient(X,Y,Y_hat,i,k,m,p);\n",
    "                sum/=n;\n",
    "                alpha[m][p]=ann.alpha[m][p]-ann.eta*sum;\n",
    "        \n",
    "        ann.alpha=alpha;\n",
    "        ann.beta=beta;\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now here we will train our model\n",
    "def train(operation,epochs,size):\n",
    "\n",
    "    #generating data\n",
    "    X,Y=gen_data(operation,size)\n",
    "    # print(f'X={X}');\n",
    "    # print(f'Y={Y}');\n",
    "    #splitting data\n",
    "    X_train,Y_train,X_test,Y_test=split_data(X,Y)\n",
    "    # print(f'X={X_train}');\n",
    "    # print(f'Y={Y_train}');\n",
    "    \n",
    "    # print(f'X={X_test}');\n",
    "    # print(f'Y={Y_test}');\n",
    "\n",
    "    #declaring loss and accuracy arrays\n",
    "    loss_train,accuracy_train,loss_test,accuracy_test=[],[],[],[];\n",
    "    \n",
    "    #initialising ann\n",
    "    ann1=ANN();\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "\n",
    "        #now getting value for training data->loss/train\n",
    "        Y_hat_train=ann1.output_array(X_train)\n",
    "        loss=Loss(Y_train,Y_hat_train)\n",
    "        accuracy=Accuracy(Y_train,Y_hat_train)\n",
    "\n",
    "        loss_train.append(loss);\n",
    "        accuracy_train.append(accuracy);\n",
    "\n",
    "        #now getting value for test data->loss/train\n",
    "        Y_hat_test=ann1.output_array(X_test)\n",
    "        loss=Loss(Y_test,Y_hat_test)\n",
    "        accuracy=Accuracy(Y_test,Y_hat_test)\n",
    "\n",
    "        loss_test.append(loss);\n",
    "        accuracy_test.append(accuracy);\n",
    "\n",
    "        #now moving forward-> updating our parameters\n",
    "        ann1.update(X_train,Y_train,Y_hat_train);\n",
    "        \n",
    "\n",
    "    return loss_train,accuracy_train,loss_test,accuracy_test\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[106], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#in this shell we initialize our neural network\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m xor_loss_train,xor_accuracy_train,xor_loss_test,xor_accuracy_test\u001b[38;5;241m=\u001b[39mtrain(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXOR\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;241m100\u001b[39m,\u001b[38;5;241m60\u001b[39m);\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxor_loss_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mxor_accuracy_train\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[105], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(operation, epochs, size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(operation,epochs,size):\n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m     \u001b[38;5;66;03m#generating data\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m     X,Y\u001b[38;5;241m=\u001b[39mgen_data(operation,size)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# print(f'X={X}');\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(f'Y={Y}');\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#splitting data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     X_train,Y_train,X_test,Y_test\u001b[38;5;241m=\u001b[39msplit_data(X,Y)\n",
      "Cell \u001b[0;32mIn[98], line 5\u001b[0m, in \u001b[0;36mgen_data\u001b[0;34m(operation, size)\u001b[0m\n\u001b[1;32m      3\u001b[0m noise\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m      4\u001b[0m X\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m----> 5\u001b[0m X\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat(X)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m operation\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXOR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      7\u001b[0m     Y\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0\u001b[39m],[\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m1\u001b[39m],[\u001b[38;5;241m0\u001b[39m]])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.12/site-packages/numpy/__init__.py:324\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    319\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn the future `np.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` will be defined as the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorresponding NumPy scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> 324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtesting\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtesting\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtesting\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'float'.\n`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "#in this shell we initialize our neural network\n",
    "xor_loss_train,xor_accuracy_train,xor_loss_test,xor_accuracy_test=train(\"XOR\",100,60);\n",
    "print(f'Y={xor_loss_train}')\n",
    "print(f'X={xor_accuracy_train}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
